name: Notebook Pages

# On schedule run the notebooks on a 'data-refresh' branch and generate pages
# then raise PR to merge changes to main.

# If src or notebooks are updated, run the notebooks to generate pages
# If project or execution time grows then change flow to 
# only run the notebooks that changed or if their associated data has changed.

# If pages changed, run 

on:
  #schedule:
    # Runs at 21:00 UTC every day
    # - cron:  '0 21 * * *'

  # Have to run notebooks on main branch to refresh cache data
  push:
    branches:
      - main
    paths:
      - 'notebooks/**'

  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'notebooks/**'
      - 'src/*.py'
      - '!src/*_test.py'

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:
    # inputs:
      # dry-run:
      #   description: 'Dry Run'
      #   type: boolean
      #   required: false
      #   default: true

permissions:
  contents: write    

jobs:
  run-notebooks:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: '**/requirements*.txt'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r src/requirements.txt 
        python -m ipykernel install --user --name python3
        jupyter kernelspec list

    - name: Cache Data
      uses: actions/cache@v4
      with:
        path: data/**
        key: notebooks-data-${{ hashFiles('data/**') }}
        restore-keys: notebooks-data-

    - name: Run Notebooks
      # run standalone notebooks first, then combo notebooks that share data using magic store
      run: |
        set -e
        echo "$(nproc) processors"
        find notebooks -name "*.ipynb" -not -name "*-and-*" -print0 | xargs -0 -I % -P $(nproc) jupyter nbconvert --config notebooks/nbconvert_cfg.py --to markdown %
        find notebooks -name "*-and-*.ipynb" -print0 | xargs -0 -I % -P $(nproc) jupyter nbconvert --config notebooks/nbconvert_cfg.py --to markdown %

    - name: Hash Data
      # Update hash of data files so we force new cache when data changes
      run: |
        set -e
        find data -type f -not -name "hash.*" | sort -n | xargs md5sum > data/hash.txt
        cat data/hash.txt

    - name: Commit changes
      if: github.ref != 'refs/heads/main'
      uses: stefanzweifel/git-auto-commit-action@v5
      with:
        branch: ${{ github.head_ref }}